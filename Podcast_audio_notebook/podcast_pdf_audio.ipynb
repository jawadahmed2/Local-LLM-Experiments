{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI-Powered Podcast Creation and Optimization Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jawad/anaconda3/envs/gpu/lib/python3.12/site-packages/langgraph/checkpoint/base.py:17: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langgraph.serde.jsonplus import JsonPlusSerializer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state of the podcast at various stages\n",
    "class PodcastState(TypedDict):\n",
    "    main_text: BaseMessage\n",
    "    key_points: BaseMessage\n",
    "    script_essence: BaseMessage\n",
    "    enhanced_script: BaseMessage\n",
    "\n",
    "\n",
    "# Define the podcast workflow\n",
    "class PodcastCreationWorkflow:\n",
    "    def __init__(self, timestamp=None, api_base=\"http://localhost:11434\", api_key=None):\n",
    "        self.api_base = api_base\n",
    "        self.api_key = api_key\n",
    "        self.model = \"llama3\"  # Ollama model name\n",
    "        self.summarizer_model = self._create_ollama_model(temperature=0)\n",
    "        self.scriptwriter_model = self._create_ollama_model(temperature=0)\n",
    "        self.enhancer_model = self._create_ollama_model(temperature=0.7)\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "        # Load system prompts\n",
    "        self.summarizer_system_prompt = self.load_prompt(\"prompts/summarizer_prompt.txt\", self.timestamp)\n",
    "        self.scriptwriter_system_prompt = self.load_prompt(\"prompts/scriptwriter_prompt.txt\", self.timestamp)\n",
    "        self.enhancer_system_prompt = self.load_prompt(\"prompts/enhancer_prompt.txt\", self.timestamp)\n",
    "\n",
    "    def _create_ollama_model(self, temperature):\n",
    "        return OllamaLLM(\n",
    "            base_url=self.api_base,\n",
    "            model=self.model,\n",
    "            temperature=temperature\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def load_prompt(file_path, timestamp=None):\n",
    "        try:\n",
    "            # Try to get the current directory using __file__\n",
    "            current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        except NameError:\n",
    "            # If __file__ is not defined (e.g., in interactive environments)\n",
    "            current_dir = os.getcwd()\n",
    "\n",
    "        root_dir = os.path.dirname(current_dir)\n",
    "\n",
    "        if timestamp:\n",
    "            prompt_history_dir = os.path.join(root_dir, \"prompt_history\")\n",
    "            base_filename = os.path.basename(file_path)\n",
    "            history_file = f\"{base_filename}_{timestamp}\"\n",
    "            history_path = os.path.join(prompt_history_dir, history_file)\n",
    "\n",
    "            if os.path.exists(history_path):\n",
    "                with open(history_path, 'r', encoding='utf-8') as file:\n",
    "                    return file.read().strip()\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read().strip()\n",
    "\n",
    "    def run_summarizer(self, state: PodcastState) -> PodcastState:\n",
    "        text = state[\"main_text\"].content\n",
    "\n",
    "        if not text:\n",
    "            raise ValueError(\"The main_text content is empty.\")\n",
    "\n",
    "        print(\"Summarizing the text to extract key points...\")\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.summarizer_system_prompt),\n",
    "            (\"human\", f\"{text}\")\n",
    "        ])\n",
    "        chain = prompt | self.summarizer_model\n",
    "        response = chain.invoke({\"text\": text})\n",
    "        key_points = response.strip()\n",
    "\n",
    "        state[\"key_points\"] = HumanMessage(content=key_points)\n",
    "        return state\n",
    "\n",
    "    def run_scriptwriter(self, state: PodcastState) -> PodcastState:\n",
    "        key_points = state[\"key_points\"].content\n",
    "\n",
    "        if not key_points:\n",
    "            raise ValueError(\"No key points found to generate the script.\")\n",
    "\n",
    "        print(\"Generating script essence from key points...\")\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.scriptwriter_system_prompt),\n",
    "            (\"human\", f\"{key_points}\")\n",
    "        ])\n",
    "        chain = prompt | self.scriptwriter_model\n",
    "        response = chain.invoke({\"key_points\": key_points})\n",
    "        script_essence = response.strip()\n",
    "\n",
    "        state[\"script_essence\"] = HumanMessage(content=script_essence)\n",
    "        return state\n",
    "\n",
    "    def run_enhancer(self, state: PodcastState) -> PodcastState:\n",
    "        script_essence = state[\"script_essence\"].content\n",
    "\n",
    "        if not script_essence:\n",
    "            raise ValueError(\"No script essence found to enhance.\")\n",
    "\n",
    "        print(\"Enhancing script with dialogue and banter...\")\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.enhancer_system_prompt),\n",
    "            (\"human\", f\"{script_essence}\")\n",
    "        ])\n",
    "        chain = prompt | self.enhancer_model\n",
    "        response = chain.invoke({\"script_essence\": script_essence})\n",
    "        enhanced_script = response.strip()\n",
    "\n",
    "        state[\"enhanced_script\"] = HumanMessage(content=enhanced_script)\n",
    "        return state\n",
    "\n",
    "    def create_workflow(self) -> StateGraph:\n",
    "        workflow = StateGraph(PodcastState)\n",
    "        workflow.set_entry_point(\"summarizer\")\n",
    "        workflow.add_node(\"summarizer\", self.run_summarizer)\n",
    "        workflow.add_node(\"scriptwriter\", self.run_scriptwriter)\n",
    "        workflow.add_node(\"enhancer\", self.run_enhancer)\n",
    "        workflow.add_edge(\"summarizer\", \"scriptwriter\")\n",
    "        workflow.add_edge(\"scriptwriter\", \"enhancer\")\n",
    "        workflow.add_edge(\"enhancer\", END)\n",
    "\n",
    "        return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample podcast text file\n",
    "file_path = \"sample_podcast.txt\"  # Ensure this file is in the same directory or provide the correct path\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    podcast_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Welcome to our podcast, where we dive into the latest advancements in artificial intelligence and machine learning. Today, we're discussing how AI is transforming industries like healthcare, finance, and education.\\n\\nIn healthcare, AI-powered systems are improving diagnostic accuracy, helping doctors make better decisions. In finance, algorithms are being used to detect fraudulent transactions and manage investments more efficiently. Meanwhile, in education, AI is revolutionizing personalized learning by adapting to each student's needs.\\n\\nLater in the show, we'll talk to experts in the field and explore some of the challenges and ethical concerns surrounding the rapid development of AI. Stay tuned for a deep dive into the exciting world of AI and machine learning.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing the text to extract key points...\n",
      "Generating script essence from key points...\n",
      "Enhancing script with dialogue and banter...\n",
      "Enhanced Script:\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the PodcastCreationWorkflow\n",
    "workflow = PodcastCreationWorkflow(api_base=\"http://localhost:11434\")\n",
    "\n",
    "# Initialize the PodcastState with some text\n",
    "state = PodcastState(\n",
    "    main_text=HumanMessage(content=podcast_content),\n",
    "    key_points=None,\n",
    "    script_essence=None,\n",
    "    enhanced_script=None\n",
    ")\n",
    "\n",
    "# Run the workflow\n",
    "state = workflow.run_summarizer(state)\n",
    "state = workflow.run_scriptwriter(state)\n",
    "state = workflow.run_enhancer(state)\n",
    "\n",
    "# Display the enhanced podcast script\n",
    "print(\"Enhanced Script:\")\n",
    "dialogue_text = state[\"enhanced_script\"].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the rewritten dialogue:\n",
      "\n",
      "Host: Welcome to today's episode on the transformative power of artificial intelligence. Imagine a world where medical diagnoses are more accurate, financial transactions are secure, and personalized learning is tailored to each student's needs.\n",
      "\n",
      "Guest: That sounds like a utopian future! But how exactly is AI achieving this?\n",
      "\n",
      "Host: Well, in healthcare, AI algorithms analyze vast amounts of data to help doctors make better decisions. It's like having a superpowered assistant that can sift through medical records, identify patterns, and provide insights that human doctors might miss.\n",
      "\n",
      "Guest: That's incredible. What about finance? How does AI impact the way we manage our money?\n",
      "\n",
      "Host: In finance, AI algorithms detect fraudulent transactions and manage investments efficiently. It's like having a hawk-eyed accountant that can spot suspicious activity and make smart investment decisions in real-time.\n",
      "\n",
      "Guest: Wow, I never thought about it that way. And what about education? How is AI changing the way we learn?\n",
      "\n",
      "Host: Ah, yes! In education, AI revolutionizes personalized learning by adapting to each student's needs. It's like having a tailored tutor that can adjust the pace and content of lessons to meet individual students' strengths and weaknesses.\n",
      "\n",
      "Guest: That sounds amazing. But what about the challenges and concerns surrounding AI? I've heard whispers about job displacement, bias, and accountability.\n",
      "\n",
      "Host: Ah, yes, those are indeed important considerations. As we celebrate the benefits of AI, we must also acknowledge the limitations and challenges that come with its development. We need to address issues like job displacement, bias, and accountability head-on.\n",
      "\n",
      "Guest: Absolutely. It's crucial that we create responsible and ethical AI systems that benefit society as a whole.\n",
      "\n",
      "Host: Exactly! And that's exactly what we'll continue to do. So, Infinity, what do you think is the most exciting application of AI in your industry?\n",
      "\n",
      "Guest: Honestly, I think it's the potential to revolutionize healthcare. The idea of using AI to develop personalized treatment plans and predict patient outcomes is incredible.\n",
      "\n",
      "Host: Wow, that's fascinating! And finally, let me ask you this: As we continue to explore the transformative power of AI, what do you think is the most important thing for us to keep in mind?\n",
      "\n",
      "Guest: I think it's crucial that we prioritize ethical considerations from the ground up. We need to ensure that AI systems are designed with transparency, accountability, and fairness in mind.\n",
      "\n",
      "Host: Well said, Infinity! And on that note, thank you for joining me today to explore the transformative power of artificial intelligence.\n",
      "\n",
      "Final Question:\n",
      "So, what do you think is the most exciting application of AI in your industry? Share your thoughts!\n"
     ]
    }
   ],
   "source": [
    "print(dialogue_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8280172548814157a3b1577c49e79ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   4%|3         | 157M/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.hf.co/repos/76/1e/761e5da7cc6a43df30e6091397aa6748ca4c7558722f8f960f355f87a51b4750/4e3d407b9b3b619da184c85786c88e5e35f90f9089303e16db696ed0be477989?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1729087601&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyOTA4NzYwMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy83Ni8xZS83NjFlNWRhN2NjNmE0M2RmMzBlNjA5MTM5N2FhNjc0OGNhNGM3NTU4NzIyZjhmOTYwZjM1NWY4N2E1MWI0NzUwLzRlM2Q0MDdiOWIzYjYxOWRhMTg0Yzg1Nzg2Yzg4ZTVlMzVmOTBmOTA4OTMwM2UxNmRiNjk2ZWQwYmU0Nzc5ODk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=c29%7ELVcsFls1BwA43NSfSEHnbLP7JwPpZKTCWz7HJUGetjco3uOHRD7pf-t7kHsq0BwBtTv2qoGoMbU5WKOsA-cuKyzGr5z-LIidMb-CFoHoodNHEMn1RfDkLVxpDd-BbRluCjwbjD4mjUp8QoDlpFEN7o2ECvwBd4tfTLfV4GRSfFIrrbHQPehUnYMuNKA6sCKBtLh6BRypGd6A6J-ny-GrRPKzpqD%7EkkigLIKpC6uPuaCOxQUGzYhpMxp00ZDSWiVCndwGOAZ3JGA4gIrIbsA7%7E-ajXVIaHzwUq-bzPfaUHWiKRnmMdORTD38U7YILQP9UmcHLqVfJAwn5kKpmJg__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6096bda3d94d4e86a09e4cdcf36821d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  19%|#8        | 849M/4.49G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jawad/anaconda3/envs/gpu/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/jawad/anaconda3/envs/gpu/lib/python3.12/site-packages/transformers/models/encodec/modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a84d5f109b4ba6b13885c409f0c298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/4.91k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModel\n",
    "import scipy.io.wavfile as wavfile\n",
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "import io\n",
    "\n",
    "# Load model and processor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = AutoProcessor.from_pretrained(\"suno/bark\")\n",
    "model = AutoModel.from_pretrained(\"suno/bark\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio(text, voice_preset=\"v2/en_speaker_6\"):\n",
    "    \"\"\"\n",
    "    Generate audio using the Bark model.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to convert to speech.\n",
    "    voice_preset (str): The voice preset to use.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: The generated audio as a numpy array.\n",
    "    \"\"\"\n",
    "    inputs = processor(text, voice_preset=voice_preset)\n",
    "    speech_output = model.generate(**inputs.to(device))\n",
    "    audio_array = speech_output.cpu().numpy().squeeze()\n",
    "    return audio_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_podcast_audio(dialogue_text):\n",
    "    \"\"\"\n",
    "    Creates an audio podcast from the given dialogue text using Bark TTS.\n",
    "\n",
    "    Args:\n",
    "    dialogue_text (str): The dialogue text to convert to audio.\n",
    "\n",
    "    Returns:\n",
    "    bytes: The generated podcast audio as bytes.\n",
    "    \"\"\"\n",
    "    dialogue_pieces = dialogue_text.split('\\n')\n",
    "    audio_pieces = []\n",
    "\n",
    "    for piece in dialogue_pieces:\n",
    "        if ':' not in piece:\n",
    "            continue\n",
    "        speaker, text = piece.split(':', 1)\n",
    "        voice_preset = \"v2/en_speaker_6\" if speaker.strip() == \"Host\" else \"v2/en_speaker_9\"\n",
    "        audio_array = generate_audio(text.strip(), voice_preset)\n",
    "        audio_pieces.append(audio_array)\n",
    "\n",
    "    # Concatenate all audio pieces\n",
    "    combined_audio = np.concatenate(audio_pieces)\n",
    "\n",
    "    # Normalize audio\n",
    "    combined_audio = np.int16(combined_audio / np.max(np.abs(combined_audio)) * 32767)\n",
    "\n",
    "    # Save to BytesIO object\n",
    "    buffer = io.BytesIO()\n",
    "    wavfile.write(buffer, 24000, combined_audio)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    return buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the podcast audio\n",
    "podcast_audio = create_podcast_audio(dialogue_text)\n",
    "\n",
    "# Optionally, save the audio to a file\n",
    "with open(\"podcast_output.wav\", \"wb\") as f:\n",
    "    f.write(podcast_audio)\n",
    "print(\"Podcast audio saved as podcast_output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the audio in the notebook\n",
    "Audio(podcast_audio, rate=24000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
