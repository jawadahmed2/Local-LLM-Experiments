{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WThmiBfvW7o0"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    datasets==2.19.1 \\\n",
        "    llama-index-core==0.11.9 \\\n",
        "    llama-index-llms-openai \\\n",
        "    llama-index-utils-workflow==0.2.1 \\\n",
        "    serpapi==0.1.5 \\\n",
        "    google-search-results==2.4.2 \\\n",
        "    semantic-router[pinecone]==0.0.65"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSNvnnmjhEZx"
      },
      "source": [
        "# Knowledge Base Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZCzed9thKZm"
      },
      "source": [
        "We'll be running our agent against a knowledge base — which requires a Pinecone index to be built.\n",
        "\n",
        "You can, if needed, skip this step and replace the `search` tool with a placeholder value if wanting to quickly test the structure of the Llama Index Workflow.\n",
        "\n",
        "If you want full functionality here, you do need to run this section — but we'll make it quick."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH0zitbAhI9N"
      },
      "source": [
        "## Download a Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5UZcVZ1hO-O"
      },
      "source": [
        "The first thing we need for an agent using RAG is somewhere we want to pull knowledge from. We will use v2 of the AI ArXiv dataset, available on Hugging Face Datasets at `jamescalam/ai-arxiv2-chunks`.\n",
        "\n",
        "Note: we're using the prechunked dataset. For the raw version see `jamescalam/ai-arxiv2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LTq8wOWhesx",
        "outputId": "53a018c6-d262-413d-c4e8-8836eefe1759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'title', 'content', 'prechunk_id', 'postchunk_id', 'arxiv_id', 'references'],\n",
              "    num_rows: 209760\n",
              "})"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqyP3a0_hqPm",
        "outputId": "cc1acf23-1781-4488-b086-7bef5d452beb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '2401.04088#0',\n",
              " 'title': 'Mixtral of Experts',\n",
              " 'content': '4 2 0 2 n a J 8 ] G L . s c [ 1 v 8 8 0 4 0 . 1 0 4 2 : v i X r a # Mixtral of Experts Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, LÃ©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, ThÃ©ophile Gervet, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, William El Sayed Abstract We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.',\n",
              " 'prechunk_id': '',\n",
              " 'postchunk_id': '2401.04088#1',\n",
              " 'arxiv_id': '2401.04088',\n",
              " 'references': ['1905.07830']}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO1-zComhw3J"
      },
      "source": [
        "## Construct Knowledge Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cztd2YZ8h5VQ"
      },
      "source": [
        "Initialized the HuggingfaceEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU92MBJxiUwq",
        "outputId": "96763a73-c3ff-4036-c33f-55222295a373"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from getpass import getpass\n",
        "from semantic_router.encoders import HuggingFaceEncoder\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") or getpass(\"OpenAI API key: \")\n",
        "\n",
        "# encoder = OpenAIEncoder(name=\"text-embedding-3-small\")\n",
        "\n",
        "encoder = HuggingFaceEncoder(name = \"BAAI/bge-small-en\") # \"text-embedding-3-small\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpMLbVDLi1pC"
      },
      "source": [
        "Initialize our connection to qdrant:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import time\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize connection to Qdrant\n",
        "qdrant_url = \"http://localhost:6333\"\n",
        "client = QdrantClient(qdrant_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get dimensions of the embeddings\n",
        "dims = len(encoder([\"some random text\"])[0])\n",
        "\n",
        "index_name = \"gpt-4o-research-agent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFamwXVekeUx",
        "outputId": "d94021cc-c901-4d8c-ec94-e0b0bbd2dfc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if collection already exists\n",
        "existing_collections = client.get_collections().collections\n",
        "existing_collection_names = [collection.name for collection in existing_collections]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBzhQFERkjwN",
        "outputId": "1a7e9832-938e-4675-a7bd-fc9030a90cef"
      },
      "outputs": [],
      "source": [
        "if index_name not in existing_collection_names:\n",
        "    # If it does not exist, create collection\n",
        "    client.create_collection(\n",
        "        collection_name=index_name,\n",
        "        vectors_config=models.VectorParams(size=dims, distance=models.Distance.DOT),\n",
        "    )\n",
        "    # Wait for collection to be initialized\n",
        "    while True:\n",
        "        collection_info = client.get_collection(index_name)\n",
        "        if collection_info.status == models.CollectionStatus.GREEN:\n",
        "            break\n",
        "        time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=1408 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.DOT: 'Dot'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
          ]
        }
      ],
      "source": [
        "# View collection stats\n",
        "collection_info = client.get_collection(index_name)\n",
        "print(collection_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEP_TzsMkldk"
      },
      "source": [
        "Populate the knowledge base:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "58f0cf25a2ec4aa3a0dabcb8a2a7c36f",
            "7f441d20804b4b829d7f09484c0369cb",
            "d8a83df6fd834d128e96057e664d9162",
            "288972f4d50549edbe00ee46407fef28",
            "e3d4bdfe01214bd6a19775f15b58f651",
            "8c46848d4c7949e1abe0c8d430f67da0",
            "427c3276bb9a421eb9814711a4934876",
            "d14b8a66298b477ca6b070d99ed86a50",
            "f641d0870d9e4407b185f622c839bf21",
            "8167b2cf4ee04576bd7f5e232be09280",
            "97d3249e633b4cc6b5beb7e51c6aefdb"
          ]
        },
        "id": "SYdvU-fjkold",
        "outputId": "9f26cf1b-7c56-407a-ff7b-e1757f5e6c4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e1ad5ddf60a4ff38fbf38abf7d79743",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Convert dataset to pandas DataFrame if it's not already\n",
        "if not isinstance(dataset, pd.DataFrame):\n",
        "    data = dataset.to_pandas()\n",
        "else:\n",
        "    data = dataset\n",
        "\n",
        "data = data[:10000]\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    # find end of batch\n",
        "    i_end = min(len(data), i + batch_size)\n",
        "    # create batch\n",
        "    batch = data.iloc[i:i_end]\n",
        "\n",
        "    # create chunks\n",
        "    chunks = [f'{row[\"title\"]}: {row[\"content\"]}' for _, row in batch.iterrows()]\n",
        "\n",
        "    # create embeddings\n",
        "    embeds = encoder(chunks)\n",
        "    assert len(embeds) == (i_end - i)\n",
        "\n",
        "    # prepare points for Qdrant\n",
        "    points = [\n",
        "        models.PointStruct(\n",
        "            id=int(id),\n",
        "            vector=embed.tolist() if isinstance(embed, np.ndarray) else embed,  # Convert numpy array to list if necessary\n",
        "            payload={\n",
        "                \"title\": row[\"title\"],\n",
        "                \"content\": row[\"content\"],\n",
        "                \"arxiv_id\": row[\"arxiv_id\"],\n",
        "                \"references\": row[\"references\"].tolist() if isinstance(row[\"references\"], np.ndarray) else row[\"references\"]\n",
        "            }\n",
        "        )\n",
        "        for id, embed, (_, row) in zip(batch.index, embeds, batch.iterrows())\n",
        "    ]\n",
        "    # print(points)\n",
        "    # upsert to Qdrant\n",
        "    client.upsert(\n",
        "        collection_name=index_name,\n",
        "        points=points\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final collection info: status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> vectors_count=None indexed_vectors_count=0 points_count=10000 segments_count=8 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=384, distance=<Distance.DOT: 'Dot'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=20000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0), quantization_config=None) payload_schema={}\n"
          ]
        }
      ],
      "source": [
        "# After upserting all data, you can get collection info again to see the changes\n",
        "final_collection_info = client.get_collection(index_name)\n",
        "print(\"Final collection info:\", final_collection_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUh_C_5HhYh6"
      },
      "source": [
        "# Agent Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RS6Ow8FLmZE"
      },
      "source": [
        "## Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51qdvTLIYKL6"
      },
      "source": [
        "We define the separate tool functions. When integrating with our graph all of these will be executed using the same `run_tools` class - which we will define later.\n",
        "\n",
        "For now, let's define the functions that our agent will have access to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Gxe_NBTHLm01"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "# our regex\n",
        "abstract_pattern = re.compile(\n",
        "    r'\\s*Abstract:\\s*(.*?)\\s*',\n",
        "    re.DOTALL\n",
        ")\n",
        "\n",
        "async def fetch_arxiv(arxiv_id: str):\n",
        "    \"\"\"Gets the abstract from an ArXiv paper given the arxiv ID. Useful for\n",
        "    finding high-level context about a specific paper.\"\"\"\n",
        "    print(\">>> fetch_arxiv\")\n",
        "    # get paper page in html\n",
        "    res = requests.get(\n",
        "        f\"https://export.arxiv.org/abs/{arxiv_id}\"\n",
        "    )\n",
        "    # search html for abstract\n",
        "    re_match = abstract_pattern.search(res.text)\n",
        "    # return abstract text\n",
        "    return re_match.group(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXmNfv63L2Dw"
      },
      "source": [
        "## Web Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpRQvPiFLvF9"
      },
      "source": [
        "The web search tool will provide the agent with access to web search. It will be instructed to use this for more general knowledge queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRLxR_eKLvgz",
        "outputId": "c7685b9a-e98d-4411-9ee3-c9ada913b1e4"
      },
      "outputs": [],
      "source": [
        "from serpapi import GoogleSearch\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "serpapi_params = {\n",
        "    \"engine\": \"google\",\n",
        "    \"api_key\": os.getenv(\"SERPAPI_KEY\") or getpass(\"SerpAPI key: \")\n",
        "}\n",
        "\n",
        "async def web_search(query: str):\n",
        "    \"\"\"Finds general knowledge information using Google search. Can also be used\n",
        "    to augment more 'general' knowledge to a previous specialist query.\"\"\"\n",
        "    print(\">>> web_search\")\n",
        "    search = GoogleSearch({\n",
        "        **serpapi_params,\n",
        "        \"q\": query,\n",
        "        \"num\": 5\n",
        "    })\n",
        "    results = search.get_dict()[\"organic_results\"]\n",
        "    contexts = \"\\n---\\n\".join(\n",
        "        [\"\\n\".join([x[\"title\"], x[\"snippet\"], x[\"link\"]]) for x in results]\n",
        "    )\n",
        "    return contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeckltzgL92n"
      },
      "source": [
        "## Rag Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEU4NDWlL-Ox"
      },
      "source": [
        "We provide two RAG-focused tools for our agent. The `rag_search` allows the agent to perform a simple RAG search for some information across all indexed research papers. The `rag_search_filter` also searches, but within a specific paper which is filtered for via the `arxiv_id` parameter.\n",
        "\n",
        "We also define the `format_rag_contexts` function to handle the transformation of our Pinecone results from a JSON object to a readble plaintext format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kuv6bK99L-no"
      },
      "outputs": [],
      "source": [
        "def format_rag_contexts(matches: list):\n",
        "    contexts = []\n",
        "    for x in matches:\n",
        "        text = (\n",
        "            f\"Title: {x['metadata']['title']}\\n\"\n",
        "            f\"Content: {x['metadata']['content']}\\n\"\n",
        "            f\"ArXiv ID: {x['metadata']['arxiv_id']}\\n\"\n",
        "            f\"Related Papers: {x['metadata']['references']}\\n\"\n",
        "        )\n",
        "        contexts.append(text)\n",
        "    context_str = \"\\n---\\n\".join(contexts)\n",
        "    return context_str\n",
        "\n",
        "async def rag_search_filter(query: str, arxiv_id: str):\n",
        "    \"\"\"Finds information from our ArXiv database using a natural language query\n",
        "    and a specific ArXiv ID. Allows us to learn more details about a specific paper.\"\"\"\n",
        "    print(\">>> rag_search_filter\")\n",
        "    xq = await encoder.acall([query])\n",
        "    xc = index_name.query(vector=xq, top_k=6, include_metadata=True, filter={\"arxiv_id\": arxiv_id})\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str\n",
        "\n",
        "async def rag_search(query: str):\n",
        "    \"\"\"Finds specialist information on AI using a natural language query.\"\"\"\n",
        "    print(\">>> rag_search\")\n",
        "    xq = await encoder.acall([query])\n",
        "    xc = index_name.query(vector=xq, top_k=2, include_metadata=True)\n",
        "    context_str = format_rag_contexts(xc[\"matches\"])\n",
        "    return context_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OGsCy9ZMVJmy"
      },
      "outputs": [],
      "source": [
        "async def final_answer(\n",
        "    introduction: str,\n",
        "    research_steps: str,\n",
        "    main_body: str,\n",
        "    conclusion: str,\n",
        "    sources: str\n",
        "):\n",
        "    \"\"\"Returns a natural language response to the user in the form of a research\n",
        "    report. There are several sections to this report, those are:\n",
        "    - `introduction`: a short paragraph introducing the user's question and the\n",
        "    topic we are researching.\n",
        "    - `research_steps`: a few bullet points explaining the steps that were taken\n",
        "    to research your report.\n",
        "    - `main_body`: this is where the bulk of high quality and concise\n",
        "    information that answers the user's question belongs. It is 3-4 paragraphs\n",
        "    long in length.\n",
        "    - `conclusion`: this is a short single paragraph conclusion providing a\n",
        "    concise but sophisticated view on what was found.\n",
        "    - `sources`: a bulletpoint list provided detailed sources for all information\n",
        "    referenced during the research process\n",
        "    \"\"\"\n",
        "    print(\">>> final_answer\")\n",
        "    if type(research_steps) is list:\n",
        "        research_steps = \"\\n\".join([f\"- {r}\" for r in research_steps])\n",
        "    if type(sources) is list:\n",
        "        sources = \"\\n\".join([f\"- {s}\" for s in sources])\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUm29WrkVNYh"
      },
      "source": [
        "## Oracle LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqM2mkxsVOoG"
      },
      "source": [
        "Our prompt for the Oracle will emphasize it's decision making ability within the `system_prompt`, leave a placeholder for us to later insert `chat_history`, and provide a place for us to insert the user `input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AW96-NkUVN5u"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"You are the oracle, the great AI decision maker.\n",
        "Given the user's query you must decide what to do with it based on the\n",
        "list of tools provided to you.\n",
        "\n",
        "If you see that a tool has been used (in the scratchpad) with a particular\n",
        "query, do NOT use that same tool with the same query again. Also, do NOT use\n",
        "any tool more than twice (ie, if the tool appears in the scratchpad twice, do\n",
        "not use it again).\n",
        "\n",
        "You should aim to collect information from a diverse range of sources before\n",
        "providing the answer to the user. Once you have collected plenty of information\n",
        "to answer the user's question (stored in the scratchpad) use the final_answer\n",
        "tool.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L59sEakdMd2"
      },
      "source": [
        "The oracle agent will be provided the tools we previously built."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sELUig_VdXK7"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "tools = [\n",
        "    FunctionTool.from_defaults(fn=fetch_arxiv),\n",
        "    FunctionTool.from_defaults(fn=web_search),\n",
        "    FunctionTool.from_defaults(fn=rag_search_filter),\n",
        "    FunctionTool.from_defaults(fn=rag_search),\n",
        "    FunctionTool.from_defaults(fn=final_answer),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### install llama index ollam\n",
        "\n",
        "- %pip install llama-index-llms-ollama\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LNiTGVrMdtN4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from llama_index.llms.ollama import Ollama\n",
        "\n",
        "\n",
        "llm = Ollama(model=\"llama3:latest\", request_timeout=120.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcXeJrFZa9eG"
      },
      "source": [
        "## Events"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzCsuWEsbAuS"
      },
      "source": [
        "We need to create a few events for our workflow. Llama-index comes with a few predefined event types, two of which we will use (`StartEvent` and `StopEvent`). However, we need to define a few additional custom event types - these are:\n",
        "\n",
        "* `InputEvent` to handle new messages and prepare chat history.\n",
        "\n",
        "* `ToolCallEvent` to trigger tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "FXdBlptmbAQk"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import ToolSelection, ToolOutput\n",
        "from llama_index.core.workflow import Event\n",
        "\n",
        "\n",
        "class InputEvent(Event):\n",
        "    input: list[ChatMessage]\n",
        "\n",
        "\n",
        "class ToolCallEvent(Event):\n",
        "    id: str\n",
        "    name: str\n",
        "    params: dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9aE7MiznZCj"
      },
      "source": [
        "Now we build the workflow. Workflows consist of a single `Workflow` class with multiple `steps`. Each step is like a compute/execution step in our agentic flow.\n",
        "\n",
        "We control which step is triggered by using different `Event` types. Each step consumes a different type of event, like `InputEvent` or `ToolCallEvent`. Additionally, our workflow begins and ends with the `StartEvent` and `StopEvent` events respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qXq1QbvScZeZ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import Workflow, StartEvent, StopEvent, step\n",
        "from llama_index.core.memory import ChatMemoryBuffer\n",
        "from llama_index.core.llms import MessageRole\n",
        "\n",
        "class ResearchAgent(Workflow):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *args: any,\n",
        "        oracle: Ollama,\n",
        "        tools: list[FunctionTool],\n",
        "        timeout: int = 20,\n",
        "    ):\n",
        "        super().__init__(*args)\n",
        "        self._timeout = timeout\n",
        "        self.oracle = oracle\n",
        "        self.tools = tools\n",
        "        self.get_tool = {tool.metadata.get_name(): tool for tool in self.tools}\n",
        "        # initialize chat history/memory with system prompt\n",
        "        self.sys_msg = ChatMessage(\n",
        "            role=MessageRole.SYSTEM,\n",
        "            content=system_prompt\n",
        "        )\n",
        "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
        "\n",
        "    @step\n",
        "    async def prepare_chat_history(self, ev: StartEvent) -> InputEvent:\n",
        "        # clear memory\n",
        "        self.memory = ChatMemoryBuffer.from_defaults(llm=llm)\n",
        "        self.memory.put(message=self.sys_msg)\n",
        "        # get user input\n",
        "        user_input = ev.input\n",
        "        user_msg = ChatMessage(role=\"user\", content=user_input)\n",
        "        self.memory.put(message=user_msg)\n",
        "        # get chat history\n",
        "        chat_history = self.memory.get()\n",
        "        # return input event\n",
        "        return InputEvent(input=chat_history)\n",
        "\n",
        "    @step\n",
        "    async def handle_llm_input(self, ev: InputEvent) -> ToolCallEvent | StopEvent:\n",
        "        chat_history = ev.input\n",
        "        # get oracle response\n",
        "        response = await self.oracle.achat_with_tools(\n",
        "            tools=self.tools,\n",
        "            chat_history=chat_history,\n",
        "            tool_choice=\"required\",\n",
        "        )\n",
        "        # add response to chat history / memory\n",
        "        self.memory.put(message=response.message)\n",
        "        # get tool calls\n",
        "        tool_calls = self.oracle.get_tool_calls_from_response(\n",
        "            response\n",
        "        )\n",
        "        # if final_answer tool used we return to the user with the StopEvent\n",
        "        if tool_calls[-1].tool_name == \"final_answer\":\n",
        "            return StopEvent(result={\"response\": tool_calls[-1].tool_kwargs})\n",
        "        else:\n",
        "            # return tool call event\n",
        "            return ToolCallEvent(\n",
        "                id=tool_calls[-1].tool_id,\n",
        "                name=tool_calls[-1].tool_name,\n",
        "                params=tool_calls[-1].tool_kwargs,\n",
        "            )\n",
        "\n",
        "    @step\n",
        "    async def run_tool(self, ev: ToolCallEvent) -> InputEvent:\n",
        "        tool_name = ev.name\n",
        "        additional_kwargs = {\n",
        "            \"tool_call_id\": ev.id,\n",
        "            \"name\": tool_name\n",
        "        }\n",
        "        # get chosen tool\n",
        "        tool = self.get_tool.get(tool_name)\n",
        "        if not tool:\n",
        "            tool_msg = ChatMessage(\n",
        "                role=\"tool\",\n",
        "                content=f\"Tool {tool_name} not found\",\n",
        "                additional_kwargs=additional_kwargs\n",
        "            )\n",
        "        else:\n",
        "            # now call tool\n",
        "            tool_output = await tool.acall(**ev.params)\n",
        "            tool_msg = ChatMessage(\n",
        "                role=\"tool\",\n",
        "                content=tool_output.content,\n",
        "                additional_kwargs=additional_kwargs\n",
        "            )\n",
        "        self.memory.put(message=tool_msg)\n",
        "        chat_history = self.memory.get()\n",
        "        return InputEvent(input=chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXs5pRIg4El9",
        "outputId": "3e5ff233-83e0-4345-c63b-e72753a916b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_21561/2290517788.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) draw_all_possible_flows. (Install `llama-index-utils-workflow` and use the import `from llama_index.utils.workflow` instead.)\n",
            "  draw_all_possible_flows(ResearchAgent, filename=\"research_agent.html\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "research_agent.html\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core.workflow import draw_all_possible_flows\n",
        "\n",
        "draw_all_possible_flows(ResearchAgent, filename=\"research_agent.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO99bN2pnLOB"
      },
      "source": [
        "Initialize the workflow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gVYXBk6SnEYE"
      },
      "outputs": [],
      "source": [
        "agent = ResearchAgent(\n",
        "    oracle=llm,\n",
        "    tools=tools,\n",
        "    timeout=130,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ynhR2BTqRim",
        "outputId": "30c3a385-57d6-4136-cbdf-1e51d5e06466"
      },
      "outputs": [],
      "source": [
        "res = await agent.run(input=\"tell me about AI\")\n",
        "res[\"response\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Looyvi2I4gvc"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBT8Wtgu6ax9"
      },
      "source": [
        "Let's test with async. To avoid overwriting state with asynchronous runs taking place we will initialize three new agent instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6qeFbqCQ8RJ3"
      },
      "outputs": [],
      "source": [
        "agent1 = ResearchAgent(oracle=llm, tools=tools, timeout=30)\n",
        "agent2 = ResearchAgent(oracle=llm, tools=tools, timeout=30)\n",
        "agent3 = ResearchAgent(oracle=llm, tools=tools, timeout=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgwZddGK6PaX",
        "outputId": "7992d327-b8d5-46f5-cf77-da444e46f6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> web_search\n",
            ">>> web_search\n",
            ">>> web_search\n",
            ">>> web_search\n",
            ">>> rag_search\n",
            ">>> rag_search\n",
            ">>> rag_search\n",
            ">>> fetch_arxiv\n",
            ">>> fetch_arxiv\n",
            ">>> fetch_arxiv\n",
            ">>> rag_search\n",
            ">>> fetch_arxiv\n",
            ">>> rag_search_filter\n",
            ">>> rag_search_filter\n",
            ">>> rag_search_filter\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "\n",
        "calls = [\n",
        "    agent.run(input=\"tell me about AI\"),\n",
        "    agent1.run(input=\"tell me about AI\"),\n",
        "    agent2.run(input=\"what is RAG?\"),\n",
        "    agent3.run(input=\"what is the latest LLM from OpenAI?\")\n",
        "]\n",
        "\n",
        "outputs = await asyncio.gather(*calls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYCUOBok66YU",
        "outputId": "59d9a095-71b6-4641-d894-eb7875633360"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': {'introduction': 'Artificial Intelligence (AI) continues to evolve rapidly, with 2023 witnessing significant advancements and trends that are shaping the future of technology. This report delves into the latest AI research trends of 2023, focusing on the rise of generative AI, the development of autonomous cognitive entities, and other key areas.',\n",
              "  'research_steps': '- Conducted a web search to identify the latest AI research trends in 2023.\\n- Utilized RAG search to gather specialist information on AI trends.\\n- Retrieved abstracts and detailed information from specific ArXiv papers related to generative AI and autonomous cognitive entities.\\n- Analyzed and synthesized the collected information to provide a comprehensive overview.',\n",
              "  'main_body': 'In 2023, the field of AI has been marked by several groundbreaking trends, with generative AI taking center stage. Generative AI, which involves creating new content from existing data, has seen explosive growth. Tools like ChatGPT and Claude have demonstrated the potential of large language models (LLMs) to generate human-like text, leading to widespread adoption in various industries. These models are not only enhancing user interactions but also driving innovations in content creation, customer service, and more.\\n\\nAnother significant trend is the development of autonomous cognitive entities. The Conceptual Framework for Autonomous Cognitive Entities (ACE) introduces a novel cognitive architecture that enables machines and software agents to operate more independently. This framework leverages the capabilities of the latest generative AI technologies, including LLMs and multimodal generative models (MMMs), to build autonomous, agentic systems. The ACE framework comprises six layers, each playing a distinct role in setting moral compasses, strategic thinking, task selection, and execution. This development is poised to revolutionize the way autonomous systems are designed and implemented.\\n\\nAdditionally, the integration of AI in various sectors continues to expand. AI-powered healthcare solutions, computer vision advancements, and AI-driven cybersecurity measures are becoming increasingly prevalent. These applications highlight the versatility of AI technologies and their potential to address complex challenges across different domains.\\n\\nThe research also underscores the importance of ethical considerations and safety in AI development. As AI systems become more autonomous and capable, ensuring their alignment with human values and societal norms is crucial. Researchers are actively exploring ways to embed ethical principles into AI frameworks to mitigate potential risks and enhance the trustworthiness of AI systems.',\n",
              "  'conclusion': 'The AI landscape in 2023 is characterized by remarkable advancements in generative AI and the emergence of autonomous cognitive entities. These trends are driving innovation across various industries and reshaping the future of technology. As AI continues to evolve, addressing ethical considerations and ensuring the safe deployment of AI systems will be paramount.',\n",
              "  'sources': \"- McKinsey Global Survey on AI, 2023\\n- Technology Review, 'Four trends that changed AI in 2023'\\n- ArXiv papers: 'SAPIEN: Affective Virtual Agents Powered by Large Language Models' (2308.03022), 'Conceptual Framework for Autonomous Cognitive Entities' (2310.06775)\"}}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNw2XibX8v5p",
        "outputId": "fe20f2a6-9404-47b6-e70b-016127179038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': {'introduction': 'Artificial Intelligence (AI) is a transformative field of computer science focused on creating systems capable of performing tasks that typically require human intelligence. This includes activities such as learning, reasoning, problem-solving, perception, and language understanding.',\n",
              "  'research_steps': '1. Conducted a web search to gather general information about AI.\\n2. Performed a specialized search in AI literature to obtain detailed insights.\\n3. Retrieved and reviewed abstracts and content from relevant ArXiv papers.\\n4. Synthesized information from multiple sources to provide a comprehensive overview.',\n",
              "  'main_body': \"Artificial Intelligence (AI) is a broad and dynamic field within computer science that aims to create machines capable of intelligent behavior. The concept of AI dates back to ancient philosophical discussions about the nature of intelligence and the possibility of non-human entities possessing it. In the 1950s, Alan Turing formalized the idea with the Turing Test, which evaluates a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.\\n\\nAI systems are often referred to as 'agents,' which are entities capable of perceiving their environment through sensors, making decisions, and taking actions using actuators. These agents can range from simple rule-based systems to complex neural networks that learn and adapt over time. The development of AI has gone through several stages, from symbolic AI, which relies on explicit rules and logic, to modern approaches like machine learning and deep learning, which use vast amounts of data to train models.\\n\\nOne of the most significant advancements in AI is the development of large language models, which can understand and generate human-like text. These models are based on neural networks and have shown remarkable capabilities in tasks such as translation, summarization, and even creative writing. The potential applications of AI are vast, spanning industries such as healthcare, finance, transportation, and entertainment.\\n\\nDespite its rapid progress, AI also raises important ethical and societal questions. Issues such as bias in AI algorithms, the impact on employment, and the need for transparency and accountability in AI systems are critical areas of ongoing research and debate.\",\n",
              "  'conclusion': 'AI is a rapidly evolving field that holds great promise for transforming various aspects of society. While it offers numerous benefits, it also presents challenges that need to be addressed through careful research and thoughtful implementation. As AI continues to advance, it will be essential to balance innovation with ethical considerations to ensure its positive impact on the world.',\n",
              "  'sources': '- Google Cloud: What is Artificial Intelligence?\\n- IBM: What is Artificial Intelligence?\\n- TechTarget: Artificial Intelligence Explained\\n- ArXiv Paper: The Rise and Potential of Large Language Model Based Agents: A Survey (2309.07864)'}}"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-4-rjVz87IS",
        "outputId": "eaf90697-b6a2-458b-84bd-e5299658c801"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': {'introduction': 'Retrieval-Augmented Generation (RAG) is an advanced AI framework that enhances the capabilities of generative models by integrating external knowledge sources. This approach aims to improve the accuracy and reliability of AI-generated content by leveraging both the inherent knowledge of the language model and additional information retrieved from external databases.',\n",
              "  'research_steps': '- Conducted a web search to gather general information about RAG.\\n- Performed a specialized RAG search to obtain detailed insights.\\n- Retrieved and analyzed an ArXiv paper (ID: 2308.03983) to understand the technical aspects of RAG.\\n- Filtered the ArXiv paper for specific details about RAG.',\n",
              "  'main_body': 'Retrieval-Augmented Generation (RAG) is a technique designed to enhance the performance of generative AI models by incorporating external knowledge sources. Traditional generative models rely solely on their pre-trained knowledge, which can sometimes lead to inaccuracies or hallucinations. RAG addresses this limitation by integrating a retrieval mechanism that fetches relevant information from external databases or knowledge bases, thereby augmenting the model\\'s responses with up-to-date and contextually relevant data.\\n\\nThe core idea behind RAG is to allow the language model to access and utilize external information dynamically. This is achieved through a two-step process: retrieval and generation. In the retrieval step, the model identifies and retrieves pertinent information from an external source based on the input query. In the generation step, the model combines this retrieved information with its inherent knowledge to produce a more accurate and contextually appropriate response.\\n\\nA study detailed in the ArXiv paper \"SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool\" (ID: 2308.03983) compares RAG with other approaches like Retrieval-Centric Generation (RCG) and Retrieval-OFF Generation (ROG). The findings indicate that RAG allows for a more flexible integration of the language model\\'s inherent knowledge with externally retrieved data. This flexibility can sometimes lead to partially erroneous information, but it generally enhances the model\\'s ability to provide accurate and relevant responses. The study also highlights the importance of prompt engineering in optimizing the performance of RAG-based systems.\\n\\nRAG\\'s ability to dynamically incorporate external knowledge makes it particularly useful in applications requiring up-to-date information, such as question answering, customer support, and content generation. By leveraging external databases, RAG can provide more reliable and contextually accurate responses, thereby improving the overall user experience.',\n",
              "  'conclusion': 'Retrieval-Augmented Generation (RAG) represents a significant advancement in the field of generative AI. By integrating external knowledge sources, RAG enhances the accuracy and reliability of AI-generated content. While it has its challenges, such as the potential for partially erroneous information, the benefits of improved contextual relevance and up-to-date responses make RAG a valuable tool in various AI applications.',\n",
              "  'sources': '- AWS: What is Retrieval-Augmented Generation?\\n- Google Cloud: What Is Retrieval Augmented Generation (RAG)?\\n- NVIDIA Blog: What Is Retrieval-Augmented Generation aka RAG\\n- ArXiv Paper: SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool (ID: 2308.03983)'}}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DStj2Zu89ad",
        "outputId": "3c9c0aff-7c81-4938-88c9-49362b864ac9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'response': {'introduction': \"OpenAI has been at the forefront of developing advanced large language models (LLMs) that push the boundaries of artificial intelligence. The latest model from OpenAI, as of 2023, is known as 'o1'.\",\n",
              "  'research_steps': \"1. Conducted a web search to find the latest information on OpenAI's LLMs.\\n2. Reviewed multiple sources to confirm the details about the latest model.\\n3. Compiled the information into a concise report.\",\n",
              "  'main_body': \"OpenAI's latest large language model, introduced in 2023, is called 'o1'. This model represents a significant advancement in the field of AI, particularly in its ability to perform complex reasoning tasks. The 'o1' model has been trained using reinforcement learning techniques, which enable it to think before responding, thereby improving its inferencing capabilities. This makes 'o1' particularly adept at handling complex, multi-step tasks that require a higher level of cognitive processing compared to its predecessors.\\n\\nIn addition to 'o1', OpenAI has also introduced another model named 'GPT-4o'. This model is designed to be a high-intelligence flagship model for complex tasks, offering improvements in speed and cost-efficiency over previous models like GPT-4 Turbo. 'GPT-4o' can handle up to 128,000 tokens, making it suitable for extensive and intricate tasks.\\n\\nThese advancements highlight OpenAI's commitment to pushing the boundaries of what is possible with LLMs. The introduction of 'o1' and 'GPT-4o' not only enhances the capabilities of AI but also sets new standards for future developments in the field.\",\n",
              "  'conclusion': \"OpenAI's latest LLMs, 'o1' and 'GPT-4o', represent significant strides in AI technology, particularly in complex reasoning and task handling. These models are set to redefine the capabilities and applications of AI in various domains.\",\n",
              "  'sources': \"- OpenAI official website\\n- Psychology Today article on OpenAI's latest LLM\\n- OpenAI API documentation\"}}"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-2YzSQA9A8L"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "288972f4d50549edbe00ee46407fef28": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8167b2cf4ee04576bd7f5e232be09280",
            "placeholder": "​",
            "style": "IPY_MODEL_97d3249e633b4cc6b5beb7e51c6aefdb",
            "value": " 79/79 [03:34&lt;00:00,  1.89s/it]"
          }
        },
        "427c3276bb9a421eb9814711a4934876": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58f0cf25a2ec4aa3a0dabcb8a2a7c36f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f441d20804b4b829d7f09484c0369cb",
              "IPY_MODEL_d8a83df6fd834d128e96057e664d9162",
              "IPY_MODEL_288972f4d50549edbe00ee46407fef28"
            ],
            "layout": "IPY_MODEL_e3d4bdfe01214bd6a19775f15b58f651"
          }
        },
        "7f441d20804b4b829d7f09484c0369cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c46848d4c7949e1abe0c8d430f67da0",
            "placeholder": "​",
            "style": "IPY_MODEL_427c3276bb9a421eb9814711a4934876",
            "value": "100%"
          }
        },
        "8167b2cf4ee04576bd7f5e232be09280": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c46848d4c7949e1abe0c8d430f67da0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97d3249e633b4cc6b5beb7e51c6aefdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d14b8a66298b477ca6b070d99ed86a50": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8a83df6fd834d128e96057e664d9162": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d14b8a66298b477ca6b070d99ed86a50",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f641d0870d9e4407b185f622c839bf21",
            "value": 79
          }
        },
        "e3d4bdfe01214bd6a19775f15b58f651": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f641d0870d9e4407b185f622c839bf21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
